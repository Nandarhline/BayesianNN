{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from os import walk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laod  data \n",
    "test_input = pd.read_pickle('DATA/test_input')\n",
    "test_output = pd.read_pickle('DATA/test_output')\n",
    "index = test_input.columns\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('Weights/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(test_input), columns=test_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = test_output/10**6  #  change of units, outputn is still a daraframe with time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions_pbnn(model, samples):\n",
    "    prediction_distribution= model(samples)\n",
    "    prediction_mean = np.squeeze(prediction_distribution.mean().numpy())/10\n",
    "    prediction_stdv = np.squeeze(prediction_distribution.stddev().numpy())/10\n",
    "\n",
    "    # The 95% CI is computed as mean Â± (1.96 * stdv)\n",
    "    upper = (prediction_mean + (1.96 * prediction_stdv))\n",
    "    lower = (prediction_mean - (1.96 * prediction_stdv))\n",
    "\n",
    "    return prediction_mean, prediction_stdv, upper, lower\n",
    "\n",
    "def loglikelihood(y, loc, scale):\n",
    "    dist = tfp.distributions.Normal(loc, scale)\n",
    "    return dist.log_prob(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the models to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_wave = 'no'\n",
    "modelIDs = [0, 1, 2, 3, 4, 5]\n",
    "# modelIDs = [5]\n",
    "# (0 for SCADA only, 1 for SCADA+Acc17, 2 for SCADA+Acc38, 3 for SCADA+Acc77, \n",
    "# 4 for SCADA+Acc17&38, 5 for SCADA+Acc17&38&77 \n",
    "\n",
    "Err_Mtl = np.zeros([len(modelIDs)])\n",
    "Err_Mtn = np.zeros([len(modelIDs)])\n",
    "for i in range(len(modelIDs)):\n",
    "    modelID = modelIDs[i]\n",
    "    \n",
    "    # Retrive features based on the modelID\n",
    "    index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "    if include_wave == 'yes': \n",
    "        index1 = index1.append(index[0:3])\n",
    "    if modelID == 1: # Acc17\n",
    "        index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "    if modelID == 2: # Acc38\n",
    "        index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "    if modelID == 3: # Acc77\n",
    "        index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "    if modelID == 4: # Acc17&38\n",
    "        index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "    if modelID == 5: # Acc17&38&77\n",
    "        index1 = index1.append(index[3:21])\n",
    "    index1 = index1.append(index[21:]) # SCADA\n",
    "    X = inputn[index1].values\n",
    "    Y = outputn.values\n",
    "    \n",
    "    def NLL(y, distr): \n",
    "      return -distr.log_prob(y) \n",
    "\n",
    "    def normal_sp(params): \n",
    "      return tfp.distributions.Normal(loc=params[:,0:2], scale=1e-3 \n",
    "                                  + tf.math.softplus(0.05 * params[:,2:4]))# both parameters are learnable\n",
    "\n",
    "    kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "    bias_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "\n",
    "    hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(inputs)\n",
    "    hidden = tfp.layers.DenseFlipout(64,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "    hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "    params = tfp.layers.DenseFlipout(4,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn)(hidden)\n",
    "    dist = tfp.layers.DistributionLambda(normal_sp)(params)\n",
    "\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=dist)\n",
    "    model.compile(Adam(learning_rate=0.0002), loss=NLL) \n",
    "    model_params = Model(inputs=inputs, outputs=params)\n",
    "    model.summary()\n",
    "    \n",
    "    # Loading the model to evaluate\n",
    "    file = h5py.File('Weights/01_SensorPlacementTest/BNNModel%s_IncWave%s.h5' % (modelID, include_wave), 'r')\n",
    "    weight = []\n",
    "    for k in range(len(file.keys())):\n",
    "        weight.append(file['weight' + str(k)][:])\n",
    "    model.set_weights(weight)\n",
    "    \n",
    "    # Prediction and Compute Error\n",
    "    Nll_Mtl = np.zeros([len(X)])\n",
    "    Nll_Mtn = np.zeros([len(X)])\n",
    "    nsim = 1000\n",
    "    for j in range(nsim):\n",
    "        prediction_mean, prediction_stdv, upper, lower = compute_predictions_pbnn(model, X)\n",
    "        nll = loglikelihood(Y, prediction_mean, prediction_stdv)\n",
    "        # running average over simulations\n",
    "        Nll_Mtl += nll[:,0]/nsim\n",
    "        Nll_Mtn += nll[:,1]/nsim\n",
    "    \n",
    "    # Average over testset\n",
    "    Err_Mtl[i] = np.mean(Nll_Mtl)\n",
    "    Err_Mtn[i] = np.mean(Nll_Mtn)\n",
    "    print(Err_Mtl[i])\n",
    "    print(Err_Mtn[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Err_Mtl)\n",
    "print(Err_Mtn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if include_wave == 'yes':\n",
    "    Models = ['SCADA+Wave', 'SCADA+Wave+Acc017', 'SCADA+Wave+Acc038', 'SCADA+Wave+Acc077', \n",
    "              'SCADA+Wave+Acc017,038', 'SCADA+Wave+Acc017,038,077']\n",
    "else:\n",
    "    Models = ['SCADA', 'SCADA+Acc017', 'SCADA+Acc038', 'SCADA+Acc077', \n",
    "              'SCADA+Acc017,038', 'SCADA+Acc017,038,077']\n",
    "\n",
    "cm = 1/2.54  # centimeters in inches\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "\n",
    "x = np.arange(6)*2\n",
    "fig, ax = plt.subplots(1, figsize=(8.5*cm, 8.5*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(left=0.15, right=.98, top=0.95, bottom=0.4, hspace = 0.65, wspace=0.15)\n",
    "# markers = ['--rD', '-bo']\n",
    "# for i in range(len()):\n",
    "ax.bar(x, Err_Mtl, color = '#6baed6', width=-0.8, align='edge', label = \"DEM$_{tl}$\")\n",
    "ax.bar(x, Err_Mtn,  color = '#08306b', width=0.8, align='edge', label = \"DEM$_{tn}$\")\n",
    "ax.set_ylabel(r'$\\mathrm{\\mathbf{\\mathbb{E}}}}$ [$\\mathrm{\\mathbf{\\mathcal{L}}}}$]')\n",
    "ax.set_xticks(x) \n",
    "ax.set_xticklabels(Models, horizontalalignment= 'right', rotation=45)\n",
    "ax.set_ylim([0, 4])\n",
    "# ax.set_ylim([-3, -1])\n",
    "def add_value_label(x_list,y_list):\n",
    "    for i in range(1, len(x_list)+1):\n",
    "        plt.text(x_list[i-1],y_list[i-1]+0.1,y_list[i-1], ha=\"center\", fontsize = 8)\n",
    "add_value_label(x-0.4,np.around(Err_Mtl,1))\n",
    "add_value_label(x+0.4,np.around(Err_Mtn,1))\n",
    "\n",
    "ax.legend(loc =\"upper left\", fontsize=9)\n",
    "# ax.legend(loc =\"lower left\", fontsize=9)\n",
    "# plt.grid(axis='y', color='0.95')\n",
    "fig.savefig('Figures/01_SensorPlacementTest/BNNErrors_IncWave%s.pdf' % (include_wave))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
