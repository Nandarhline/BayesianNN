{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from bayesian_models import Pbnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelID = 3\n",
    "# (0 for SCADA only, 1 for SCADA+Acc17, 2 for SCADA+Acc38, 3 for SCADA+Acc77, \n",
    "# 4 for SCADA+Acc17&38, 5 for SCADA+Acc17&38&77 \n",
    "include_wave = 'no'\n",
    "duration = '24M'\n",
    "durations = ['3M','6M','9M','12M','15M','18M','21M','24M']\n",
    "train_end_dates = ['2018-03-31 23:50:00+00:00', '2018-06-30 23:50:00+00:00', '2018-09-30 23:50:00+00:00', \n",
    "                  '2018-12-31 23:50:00+00:00', '2019-03-31 23:50:00+00:00', '2019-06-30 23:50:00+00:00',\n",
    "                  '2019-09-30 23:50:00+00:00', '2019-12-31 23:50:00+00:00']\n",
    "train_end_date = train_end_dates[durations.index(duration)]\n",
    "\n",
    "# Laod train data \n",
    "train_input = pd.read_pickle('../../DATA/train_input')\n",
    "train_output = pd.read_pickle('../../DATA/train_output')\n",
    "index = train_input.columns\n",
    "\n",
    "train_input = train_input.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "train_output= train_output.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "# print(train_input.shape)\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('../../DATA/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(train_input), columns=train_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = train_output/10**5  #  change of units, outputn is still a daraframe with time index\n",
    "\n",
    "# Retrive features based on the modelID\n",
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "X = inputn[index1].values\n",
    "Y = outputn.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"n_infeatures\": X.shape[1],\n",
    "          \"n_outfeatures\": Y.shape[1],\n",
    "          \"n_samples\": X.shape[0],\n",
    "          \"outout_dist\": \"Normal\",\n",
    "          \"learn_all_params\": True} \n",
    "\n",
    "fl_model = Pbnn(config)\n",
    "fl_model.build_bnn(3,[32,64,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = {\"optimizer\": optimizers.Adam,\n",
    "             \"learning_rate\": 0.0002,\n",
    "             \"batch_size\": 1024,\n",
    "             \"epochs\": 2000,\n",
    "             \"callback_patience\": 30,\n",
    "             \"verbose\": 1}\n",
    "fl_model.train_bnn(X,Y,train_env)\n",
    "\n",
    "# Save the weights\n",
    "# with open(\"../../DATA/Model%d_IncWave%s_%s.h5\" % (modelID, include_wave, duration), \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(fl_model.weights, fp)\n",
    "# with open(\"../../DATA/Model%d_IncWave%s_%s.h5\" % (modelID, include_wave, duration),  \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = pd.read_pickle('../../DATA/test_input')\n",
    "test_output = pd.read_pickle('../../DATA/test_output')\n",
    "index = test_input.columns\n",
    "# Sort in order of increasing wind speed\n",
    "# test_input = test_input.sort_values(by=['mean_BB_G10_windspeed'])\n",
    "# test_input = test_input.drop_duplicates(subset=['mean_BB_G10_windspeed'], keep='last')\n",
    "# test_output = test_output.reindex(test_input.index)\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('../../DATA/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(test_input), columns=test_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = test_output/10**5 #  change of units, outputn is still a daraframe with time index\n",
    "\n",
    "# Retrive features based on the modelID\n",
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "Xtest = inputn[index1].values\n",
    "Ytest = outputn.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network output wrt to the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELL_Ytest = fl_model.evaluate_bnn(Xtest, Ytest, nsim=100)\n",
    "print(np.mean(ELL_Ytest,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = Xtest[1:50,:]\n",
    "Yt = Ytest[1:50,:]\n",
    "Mean_Y, Stdv_Y = fl_model.test_bnn(Xt, nsim=100)\n",
    "\n",
    "cm = 1/2.54\n",
    "x = np.arange(Xt.shape[0])\n",
    "fig, ax = plt.subplots(2, figsize=(17*cm, 8*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(left=0.1, right=.98, top=0.98, bottom=0.15, hspace = 0.1, wspace=0.15)\n",
    "ax[0].plot(x, Mean_Y[:,0], 'r-', label='Predictive mean');\n",
    "ax[0].scatter(x, Yt[:,0], marker='+', label='Measured');\n",
    "ax[0].fill_between(x,np.squeeze(Mean_Y[:,0]+1.96*Stdv_Y[:,0]), np.squeeze(Mean_Y[:,0] -1.96*Stdv_Y[:,0]),\n",
    "                 alpha=0.5, label='95% CI (+/- 1.96std)')\n",
    "ax[0].set_yticklabels([])\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_ylabel('DEM$_{tl}$')\n",
    "ax[0].legend(ncol=3)\n",
    "\n",
    "ax[1].plot(x, Mean_Y[:,1], 'r-', label='Predictive mean');\n",
    "ax[1].scatter(x, Yt[:,1], marker='+', label='Measured');\n",
    "ax[1].fill_between(x,np.squeeze(Mean_Y[:,1]+1.96*Stdv_Y[:,1]), np.squeeze(Mean_Y[:,1] -1.96*Stdv_Y[:,1]),\n",
    "                 alpha=0.5, label='95% CI (+/- 1.96std)')\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_ylabel('DEM$_{tn}$')\n",
    "ax[1].legend(ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantify model uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_muY, Stdv_muY, Mean_sigmaY, Stdv_sigmaY = fl_model.modeluq_bnn(Xtest, nsim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute percentage error betweeen predicted means and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Percentage_error = 100*np.abs(Mean_muY-Ytest)/Ytest\n",
    "print(np.mean(Percentage_error, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
