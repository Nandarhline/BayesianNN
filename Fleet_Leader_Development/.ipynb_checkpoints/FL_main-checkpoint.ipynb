{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from bayesian_models import Pbnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../DATA/train_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_end_date \u001b[38;5;241m=\u001b[39m train_end_dates[durations\u001b[38;5;241m.\u001b[39mindex(duration)]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Laod train data \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_input \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../DATA/train_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m train_output \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../DATA/train_output\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m index \u001b[38;5;241m=\u001b[39m train_input\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\pickle.py:190\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../DATA/train_input'"
     ]
    }
   ],
   "source": [
    "modelID = 3\n",
    "# (0 for SCADA only, 1 for SCADA+Acc17, 2 for SCADA+Acc38, 3 for SCADA+Acc77, \n",
    "# 4 for SCADA+Acc17&38, 5 for SCADA+Acc17&38&77 \n",
    "include_wave = 'no'\n",
    "duration = '24M'\n",
    "durations = ['3M','6M','9M','12M','15M','18M','21M','24M']\n",
    "train_end_dates = ['2018-03-31 23:50:00+00:00', '2018-06-30 23:50:00+00:00', '2018-09-30 23:50:00+00:00', \n",
    "                  '2018-12-31 23:50:00+00:00', '2019-03-31 23:50:00+00:00', '2019-06-30 23:50:00+00:00',\n",
    "                  '2019-09-30 23:50:00+00:00', '2019-12-31 23:50:00+00:00']\n",
    "train_end_date = train_end_dates[durations.index(duration)]\n",
    "\n",
    "# Laod train data \n",
    "train_input = pd.read_pickle('../../DATA/train_input')\n",
    "train_output = pd.read_pickle('../../DATA/train_output')\n",
    "index = train_input.columns\n",
    "\n",
    "train_input = train_input.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "train_output= train_output.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "# print(train_input.shape)\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('../../DATA/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(train_input), columns=train_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = train_output/10**5  #  change of units, outputn is still a daraframe with time index\n",
    "\n",
    "# Retrive features based on the modelID\n",
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "X = inputn[index1].values\n",
    "Y = outputn.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"n_infeatures\": X.shape[1],\n",
    "          \"n_outfeatures\": Y.shape[1],\n",
    "          \"n_samples\": X.shape[0],\n",
    "          \"outout_dist\": \"Normal\",\n",
    "          \"learn_all_params\": True} \n",
    "\n",
    "fl_model = Pbnn(config)\n",
    "fl_model.build_bnn(3,[32,64,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = {\"optimizer\": optimizers.Adam,\n",
    "             \"learning_rate\": 0.0002,\n",
    "             \"batch_size\": 1024,\n",
    "             \"epochs\": 2000,\n",
    "             \"callback_patience\": 30,\n",
    "             \"verbose\": 1}\n",
    "fl_model.train_bnn(X,Y,train_env)\n",
    "\n",
    "# Save the weights\n",
    "# with open(\"../../DATA/Model%d_IncWave%s_%s.h5\" % (modelID, include_wave, duration), \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(fl_model.weights, fp)\n",
    "# with open(\"../../DATA/Model%d_IncWave%s_%s.h5\" % (modelID, include_wave, duration),  \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = pd.read_pickle('../../DATA/test_input')\n",
    "test_output = pd.read_pickle('../../DATA/test_output')\n",
    "index = test_input.columns\n",
    "# Sort in order of increasing wind speed\n",
    "# test_input = test_input.sort_values(by=['mean_BB_G10_windspeed'])\n",
    "# test_input = test_input.drop_duplicates(subset=['mean_BB_G10_windspeed'], keep='last')\n",
    "# test_output = test_output.reindex(test_input.index)\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('../../DATA/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(test_input), columns=test_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = test_output/10**5 #  change of units, outputn is still a daraframe with time index\n",
    "\n",
    "# Retrive features based on the modelID\n",
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "Xtest = inputn[index1].values\n",
    "Ytest = outputn.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network output wrt to the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELL_Ytest = fl_model.evaluate_bnn(Xtest, Ytest, nsim=100)\n",
    "print(np.mean(ELL_Ytest,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = Xtest[1:50,:]\n",
    "Yt = Ytest[1:50,:]\n",
    "Mean_Y, Stdv_Y = fl_model.test_bnn(Xt, nsim=100)\n",
    "\n",
    "cm = 1/2.54\n",
    "x = np.arange(Xt.shape[0])\n",
    "fig, ax = plt.subplots(2, figsize=(17*cm, 8*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(left=0.1, right=.98, top=0.98, bottom=0.15, hspace = 0.1, wspace=0.15)\n",
    "ax[0].plot(x, Mean_Y[:,0], 'r-', label='Predictive mean');\n",
    "ax[0].scatter(x, Yt[:,0], marker='+', label='Measured');\n",
    "ax[0].fill_between(x,np.squeeze(Mean_Y[:,0]+1.96*Stdv_Y[:,0]), np.squeeze(Mean_Y[:,0] -1.96*Stdv_Y[:,0]),\n",
    "                 alpha=0.5, label='95% CI (+/- 1.96std)')\n",
    "ax[0].set_yticklabels([])\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_ylabel('DEM$_{tl}$')\n",
    "ax[0].legend(ncol=3)\n",
    "\n",
    "ax[1].plot(x, Mean_Y[:,1], 'r-', label='Predictive mean');\n",
    "ax[1].scatter(x, Yt[:,1], marker='+', label='Measured');\n",
    "ax[1].fill_between(x,np.squeeze(Mean_Y[:,1]+1.96*Stdv_Y[:,1]), np.squeeze(Mean_Y[:,1] -1.96*Stdv_Y[:,1]),\n",
    "                 alpha=0.5, label='95% CI (+/- 1.96std)')\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_ylabel('DEM$_{tn}$')\n",
    "ax[1].legend(ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantify model uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_muY, Stdv_muY, Mean_sigmaY, Stdv_sigmaY = fl_model.modeluq_bnn(Xtest, nsim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute percentage error betweeen predicted means and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Percentage_error = 100*np.abs(Mean_muY-Ytest)/Ytest\n",
    "print(np.mean(Percentage_error, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
