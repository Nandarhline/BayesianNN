{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'turbine' # location\n",
    "modelID = 3\n",
    "# (0 for SCADA only, 1 for SCADA+Acc17, 2 for SCADA+Acc38, 3 for SCADA+Acc77, \n",
    "# 4 for SCADA+Acc17&38, 5 for SCADA+Acc17&38&77 \n",
    "include_wave = 'no'\n",
    "duration = '24M'\n",
    "\n",
    "durations = ['3M','6M','9M','12M','15M','18M','21M','24M']\n",
    "train_end_dates = ['2018-03-31 23:50:00+00:00', '2018-06-30 23:50:00+00:00', '2018-09-30 23:50:00+00:00', \n",
    "                  '2018-12-31 23:50:00+00:00', '2019-03-31 23:50:00+00:00', '2019-06-30 23:50:00+00:00',\n",
    "                  '2019-09-30 23:50:00+00:00', '2019-12-31 23:50:00+00:00']\n",
    "train_end_date = train_end_dates[durations.index(duration)]\n",
    "\n",
    "# Laod train data \n",
    "train_input = pd.read_pickle('DATA/train_input')\n",
    "train_output = pd.read_pickle('DATA/train_output')\n",
    "index = train_input.columns\n",
    "\n",
    "train_input = train_input.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "train_output= train_output.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "# print(train_input.shape)\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('Weights/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(train_input), columns=train_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = train_output/10**5  #  change of units, outputn is still a daraframe with time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "X = inputn[index1].values\n",
    "Y = outputn.values\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BNN Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(y, distr): \n",
    "  return -distr.log_prob(y) \n",
    "\n",
    "def normal_sp(params): \n",
    "  return tfp.distributions.Normal(loc=params[:,0:2], scale=1e-3 \n",
    "                                  + tf.math.softplus(0.05 * params[:,2:4]))# both parameters are learnable\n",
    "\n",
    "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "bias_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "\n",
    "hidden = tfp.layers.DenseFlipout(31,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(inputs)\n",
    "hidden = tfp.layers.DenseFlipout(64,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "params = tfp.layers.DenseFlipout(4,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn)(hidden)\n",
    "dist = tfp.layers.DistributionLambda(normal_sp)(params)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=dist)\n",
    "model.compile(Adam(learning_rate=0.0003), loss=NLL) \n",
    "\n",
    "model_params = Model(inputs=inputs, outputs=params)\n",
    "model.summary()\n",
    "\n",
    "#train the model\n",
    "epoch = 1000\n",
    "batch_size = 1024\n",
    "history1 = model.fit(X, Y, batch_size=batch_size, epochs=epoch, verbose=1, validation_split = 0.2); # To save weight history, add callback1 and/or callback2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Architecture and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = X.shape[1]\n",
    "out_dim = Y.shape[1]\n",
    "VSnet = Sequential()\n",
    "VSnet.add(Dense(64, input_dim=in_dim, activation=\"relu\"))\n",
    "VSnet.add(Dense(128, activation=\"relu\"))\n",
    "VSnet.add(Dense(64, activation=\"relu\"))\n",
    "VSnet.add(Dense(out_dim, activation=\"relu\"))\n",
    "\n",
    "loss = losses.mean_absolute_error\n",
    "optimizer = optimizers.Adamax(learning_rate = 0.001)\n",
    "VSnet.compile(loss = loss, optimizer = optimizer)\n",
    " \n",
    "VSnet.summary()\n",
    "\n",
    "history2 = VSnet.fit(X, Y, validation_split=0.2, epochs=1000, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 1/2.54  # centimeters in inches\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "# fig, ax = plt.subplots(1, figsize=(7.5*cm, 6*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "fig, ax = plt.subplots(2,1, figsize=(8.5*cm, 7.5*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "ax[0].plot(history2.history['loss'][0:700],'k',linewidth=1)\n",
    "ax[0].plot(history2.history['val_loss'][0:700],'r',linewidth=1, ls='--')\n",
    "ax[0].legend(['DNN (Training loss)', 'DNN (Validation loss)'], loc='upper right', ncol=1)\n",
    "ax[0].set_ylabel('MAE')\n",
    "ax[0].set_ylim([0.071,0.151])\n",
    "ax[0].set_xticklabels([])\n",
    "# ax2 = ax1.twinx()\n",
    "ax[1].plot(history1.history['loss'][0:700],'k',linewidth=1)\n",
    "ax[1].plot(history1.history['val_loss'][0:700],'r',linewidth=1, ls='--')\n",
    "ax[1].legend(['BNN (Training loss)', 'BNN (Validation loss)', 'DNN (Training loss)', 'DNN (Validation loss)'], loc='upper right')\n",
    "\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('-ELBO')\n",
    "ax[1].set_ylim([-1,5])\n",
    "\n",
    "# \n",
    "# \n",
    "# ax.set_title('')\n",
    "# ax.set_yscale('log')\n",
    "# ax.legend()\n",
    "# fig.show()\n",
    "plt.subplots_adjust(left=0.15, right=.98, top=0.98, bottom=0.15, hspace = 0.1, wspace=0.15)\n",
    "fig.savefig('Figures/Overfit.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
