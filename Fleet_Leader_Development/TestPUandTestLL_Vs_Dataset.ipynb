{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = pd.read_pickle('DATA/test_input')\n",
    "test_output = pd.read_pickle('DATA/test_output')\n",
    "index = test_input.columns\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('Weights/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(test_input), columns=test_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = test_output/10**6  #  change of units, outputn is still a daraframe with time index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the models to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_wave = 'no'\n",
    "modelID = 3\n",
    "# (0 for SCADA only, 1 for SCADA+Acc17, 2 for SCADA+Acc38, 3 for SCADA+Acc77, \n",
    "# 4 for SCADA+Acc17&38, 5 for SCADA+Acc17&38&77 \n",
    "\n",
    "# Retrive features based on the modelID\n",
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "X = inputn[index1].values\n",
    "Y = outputn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(y, distr): \n",
    "  return -distr.log_prob(y) \n",
    "\n",
    "def normal_sp(params): \n",
    "  return tfp.distributions.Normal(loc=params[:,0:2], scale=1e-3 \n",
    "                                  + tf.math.softplus(0.05 * params[:,2:4]))# both parameters are learnable\n",
    "\n",
    "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "bias_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "\n",
    "hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(inputs)\n",
    "hidden = tfp.layers.DenseFlipout(64,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "params = tfp.layers.DenseFlipout(4,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn)(hidden)\n",
    "dist = tfp.layers.DistributionLambda(normal_sp)(params)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=dist)\n",
    "model.compile(Adam(learning_rate=0.0002), loss=NLL) \n",
    "\n",
    "model_params = Model(inputs=inputs, outputs=params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions_pbnn(model, samples):\n",
    "    prediction_distribution= model(samples)\n",
    "    prediction_mean = np.squeeze(prediction_distribution.mean().numpy())/10\n",
    "    prediction_stdv = np.squeeze(prediction_distribution.stddev().numpy())/10\n",
    "\n",
    "    # The 95% CI is computed as mean Â± (1.96 * stdv)\n",
    "    upper = (prediction_mean + (1.96 * prediction_stdv))\n",
    "    lower = (prediction_mean - (1.96 * prediction_stdv))\n",
    "\n",
    "    return prediction_mean, prediction_stdv, upper, lower\n",
    "\n",
    "def loglikelihood(y, loc, scale):\n",
    "    dist = tfp.distributions.Normal(loc, scale)\n",
    "    return dist.log_prob(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = ['3M','6M','9M','12M','15M','18M','21M','24M']\n",
    "# durations = ['6M','12M','18M','24M']\n",
    "\n",
    "PU_Mtl = np.zeros([len(durations),len(X)])\n",
    "PU_Mtn = np.zeros([len(durations),len(X)])\n",
    "LL_Mtl = np.zeros([len(durations),len(X)])\n",
    "LL_Mtn = np.zeros([len(durations),len(X)])\n",
    "\n",
    "for ind in range(len(durations)):\n",
    "    DEM_tl = np.zeros([len(X)])\n",
    "    DEM_tn = np.zeros([len(X)])\n",
    "    DEM_tl2 = np.zeros([len(X)])\n",
    "    DEM_tn2 = np.zeros([len(X)])\n",
    "    ll_Mtl = np.zeros([len(X)])\n",
    "    ll_Mtn = np.zeros([len(X)])\n",
    "    file = h5py.File('Weights/02_PredictionModel1/Model%d_IncWave%s_%s.h5' % (modelID, include_wave, durations[ind]), 'r')\n",
    "    weight = []\n",
    "    for i in range(len(file.keys())):\n",
    "        weight.append(file['weight' + str(i)][:])\n",
    "    model.set_weights(weight)\n",
    "    file.close()\n",
    "    nsim = 1000\n",
    "    for j in range(nsim):\n",
    "#         np.random.seed(j)\n",
    "        dems = model.predict(X)\n",
    "        DEM_tl += dems[:,0]\n",
    "        DEM_tn += dems[:,1]\n",
    "        DEM_tl2 += (dems[:,0])**2\n",
    "        DEM_tn2 += (dems[:,1])**2\n",
    "        prediction_mean, prediction_stdv, upper, lower = compute_predictions_pbnn(model, X)\n",
    "        lls = loglikelihood(Y, prediction_mean, prediction_stdv)\n",
    "        ll_Mtl += lls[:,0]\n",
    "        ll_Mtn += lls[:,1]\n",
    "\n",
    "    PU_Mtl[ind, :] = np.sqrt((DEM_tl2/nsim)-(DEM_tl/nsim)**2)/(DEM_tl/nsim)\n",
    "    PU_Mtn[ind, :] = np.sqrt((DEM_tn2/nsim)-(DEM_tn/nsim)**2)/(DEM_tn/nsim)\n",
    "#     print(PU_Mtl)\n",
    "#     print(PU_Mtn)\n",
    "    LL_Mtl[ind,:] = ll_Mtl/nsim\n",
    "    LL_Mtn[ind,:] = ll_Mtn/nsim\n",
    "#     print(LL_Mtl)\n",
    "#     print(LL_Mtn)\n",
    "    print(ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestLL_Mtl = np.mean(LL_Mtl, axis = 1)\n",
    "TestLL_Mtn = np.mean(LL_Mtn, axis = 1)\n",
    "TestPU_Mtl = np.mean(PU_Mtl, axis = 1)\n",
    "TestPU_Mtn = np.mean(PU_Mtn, axis = 1)\n",
    "\n",
    "# To plot x-axis according to dataset size\n",
    "durations = ['3M','6M','9M','12M','15M','18M','21M','24M']\n",
    "# durations = ['6M','12M','18M','24M']\n",
    "train_end_dates = ['2018-03-31 23:50:00+00:00', '2018-06-30 23:50:00+00:00', '2018-09-30 23:50:00+00:00', \n",
    "                  '2018-12-31 23:50:00+00:00', '2019-03-31 23:50:00+00:00', '2019-06-30 23:50:00+00:00',\n",
    "                  '2019-09-30 23:50:00+00:00', '2019-12-31 23:50:00+00:00']\n",
    "ticks = np.zeros([len(durations)])\n",
    "train_input = pd.read_pickle('DATA/train_input')\n",
    "for i in range(len(durations)):   \n",
    "    input = train_input.loc['2018-01-01 00:00:00+00:00':train_end_dates[i]]\n",
    "    ticks[i] = len(input)\n",
    "\n",
    "cm = 1/2.54  # centimeters in inches\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "fig, ax = plt.subplots(1, figsize=(8.5*cm, 6*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(left=0.21, right=.98, top=0.98, bottom=0.25, hspace = 0.65, wspace=0.15)\n",
    "ax.plot(ticks, TestPU_Mtl, color = '#2171b5', marker = 'o', markersize = 5, ls =':', label=r'DEM$_{tl}$')\n",
    "ax.plot(ticks, TestPU_Mtn,  color = '#08306b', marker = 'o', markersize = 5, label=r'DEM$_{tn}$')\n",
    "\n",
    "ax.legend(loc = 'upper right')\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(durations, rotation= 90)\n",
    "ax.set_xlabel('Duration of data collection') \n",
    "ax.set_ylabel(r'CoV($\\hat{DEM}$)') \n",
    "plt.grid(axis='y', color='0.95')\n",
    "fig.savefig('Figures/03_PredictionModel/CoV_vs_Trainsize.pdf')\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8.5*cm, 6*cm), sharey='row', dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplots_adjust(left=0.21, right=.98, top=0.98, bottom=0.25, hspace = 0.65, wspace=0.15)\n",
    "ax.plot(ticks, TestLL_Mtl , color = '#2171b5', marker = 'o', markersize = 5, ls =':', label=r'DEM$_{tl}$')\n",
    "ax.plot(ticks, TestLL_Mtn ,  color = '#08306b', marker = 'o', markersize = 5, label=r'DEM$_{tn}$')\n",
    "ax.legend(loc = 'upper left')\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(durations, rotation= 90)\n",
    "ax.set_xlabel('Duration of data collection')  \n",
    "ax.set_ylabel(r'$\\mathbf{\\mathbb{E}}$ $[\\mathbf{\\mathcal{L}}(DEM)]$')  \n",
    "plt.grid(axis='y', color='0.95')\n",
    "fig.savefig('Figures/03_PredictionModel/LL_vs_Trainsize.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
