{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'bbg10' # location\n",
    "modelID = 5\n",
    "# (0 for SCADA only, 1 for SCADA+Acc17, 2 for SCADA+Acc38, 3 for SCADA+Acc77, \n",
    "# 4 for SCADA+Acc17&38, 5 for SCADA+Acc17&38&77 \n",
    "include_wave = 'no'\n",
    "duration = '24M'\n",
    "\n",
    "durations = ['3M','6M','9M','12M','15M','18M','21M','24M']\n",
    "train_end_dates = ['2018-03-31 23:50:00+00:00', '2018-06-30 23:50:00+00:00', '2018-09-30 23:50:00+00:00', \n",
    "                  '2018-12-31 23:50:00+00:00', '2019-03-31 23:50:00+00:00', '2019-06-30 23:50:00+00:00',\n",
    "                  '2019-09-30 23:50:00+00:00', '2019-12-31 23:50:00+00:00']\n",
    "train_end_date = train_end_dates[durations.index(duration)]\n",
    "\n",
    "# Laod train data \n",
    "train_input = pd.read_pickle('DATA/train_input')\n",
    "train_output = pd.read_pickle('DATA/train_output')\n",
    "index = train_input.columns\n",
    "\n",
    "train_input = train_input.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "train_output= train_output.loc['2018-01-01 00:00:00+00:00':train_end_date]\n",
    "# print(train_input.shape)\n",
    "\n",
    "# Normlaization of input data\n",
    "# Data normalization according to training dataset/ model\n",
    "filehandler = open('Weights/Norm', 'rb') \n",
    "std_scaler = pickle.load(filehandler)\n",
    "inputn = pd.DataFrame(std_scaler.transform(train_input), columns=train_input.columns) \n",
    "# inputn is still a daraframe with numeric index\n",
    "outputn = train_output/10**5  #  change of units, outputn is still a daraframe with time index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive features based on the modelID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = pd.core.indexes.base.Index([]) # create a blank index array\n",
    "if include_wave == 'yes': \n",
    "    index1 = index1.append(index[0:3])\n",
    "if modelID == 1: # Acc17\n",
    "    index1 = index1.append(index[[3,4,9,10,15,16]])\n",
    "if modelID == 2: # Acc38\n",
    "    index1 = index1.append(index[[5,6,11,12,17,18]])\n",
    "if modelID == 3: # Acc77\n",
    "    index1 = index1.append(index[[7,8,13,14,19,20]])\n",
    "if modelID == 4: # Acc17&38\n",
    "    index1 = index1.append(index[[3,4,5,6,9,10,11,12,15,16,17,18]])   \n",
    "if modelID == 5: # Acc17&38&77\n",
    "    index1 = index1.append(index[3:21])\n",
    "\n",
    "index1 = index1.append(index[21:]) # SCADA\n",
    "X = inputn[index1].values\n",
    "Y = outputn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(y, distr): \n",
    "  return -distr.log_prob(y) \n",
    "\n",
    "def normal_sp(params): \n",
    "  return tfp.distributions.Normal(loc=params[:,0:2], scale=1e-3 \n",
    "                                  + tf.math.softplus(0.05 * params[:,2:4]))# both parameters are learnable\n",
    "\n",
    "kernel_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "bias_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) / (X.shape[0] * 1.0)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "\n",
    "hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(inputs)\n",
    "hidden = tfp.layers.DenseFlipout(64,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "hidden = tfp.layers.DenseFlipout(32,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn,activation=\"relu\")(hidden)\n",
    "params = tfp.layers.DenseFlipout(4,bias_posterior_fn=tfp.layers.util.default_mean_field_normal_fn(),\n",
    "                           bias_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
    "                           kernel_divergence_fn=kernel_divergence_fn,\n",
    "                           bias_divergence_fn=bias_divergence_fn)(hidden)\n",
    "dist = tfp.layers.DistributionLambda(normal_sp)(params)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=dist)\n",
    "model.compile(Adam(learning_rate=0.0003), loss=NLL) \n",
    "\n",
    "model_params = Model(inputs=inputs, outputs=params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records the weights throughout the training process\n",
    "epoch_history = []\n",
    "# A custom callback\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\n",
    "class MyCallback1(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        weights = model.get_weights()\n",
    "        file = h5py.File('Weights/03_UncertaintyEvolution/%d.h5' % (epoch), 'w')\n",
    "        for i in range(len(weights)):\n",
    "           file.create_dataset('weight' + str(i), data=weights[i])\n",
    "        file.close()\n",
    "        epoch_history.append(weights)\n",
    "callback1 = MyCallback1()\n",
    "\n",
    "batch_history = []\n",
    "class MyCallback2(tf.keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        weights = model.get_weights()\n",
    "        batch_history.append(weights)\n",
    "callback2 = MyCallback2()\n",
    "\n",
    "callback3 = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1,\n",
    "    mode='auto', baseline=None, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "epoch = 2000\n",
    "batch_size = 1024\n",
    "history = model.fit(X, Y, batch_size=batch_size, epochs=epoch, verbose=1, validation_split = 0, \n",
    "          callbacks = [callback3]); # To save weight history, add callback1 and/or callback2\n",
    "\n",
    "# save the final weights\n",
    "file = h5py.File('Weights/01_SensorPlacementTest1/BNNModel%d_IncWave%s.h5' % (modelID, include_wave), 'w')\n",
    "weight = model.get_weights()\n",
    "for i in range(len(weight)):\n",
    "   file.create_dataset('weight' + str(i), data=weight[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save weight history\n",
    "# from scipy import io\n",
    "# io.savemat('Weights/02_PredictionModel1/Model%d_IncWave%s_%s_EpochHistory.mat'% (modelID, include_wave, duration), {\"weights_history\": epoch_history })\n",
    "# io.savemat('Weights/02_PredictionModel1/Model%d_IncWave%s_%s_BatchHistory.mat'% (modelID, include_wave, duration), {\"weights_history\": batch_history })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions_pbnn(model, samples):\n",
    "    prediction_distribution= model(samples)\n",
    "    prediction_mean = np.squeeze(prediction_distribution.mean().numpy())\n",
    "    prediction_stdv = np.squeeze(prediction_distribution.stddev().numpy())\n",
    "\n",
    "    # The 95% CI is computed as mean Â± (1.96 * stdv)\n",
    "    upper = (prediction_mean + (1.96 * prediction_stdv))\n",
    "    lower = (prediction_mean - (1.96 * prediction_stdv))\n",
    "  \n",
    "    return prediction_mean, prediction_stdv, upper, lower\n",
    "\n",
    "def loglikelihood(y, loc, scale):\n",
    "    dist = tfp.distributions.Normal(loc, scale)\n",
    "    return dist.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nll_Mtl = np.zeros([len(X)])\n",
    "Nll_Mtn = np.zeros([len(X)])\n",
    "for j in range(100):\n",
    "    prediction_mean, prediction_stdv, upper, lower = compute_predictions_pbnn(model, X)\n",
    "    nll = loglikelihood(Y, prediction_mean, prediction_stdv)\n",
    "    # running average over simulations\n",
    "    Nll_Mtl += nll[:,0]/100\n",
    "    Nll_Mtn += nll[:,1]/100\n",
    "\n",
    "# Average over trainset\n",
    "ECov_Mtl = np.mean(Nll_Mtl) \n",
    "ECov_Mtn = np.mean(Nll_Mtn)\n",
    "print(ECov_Mtl)\n",
    "print(ECov_Mtn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
